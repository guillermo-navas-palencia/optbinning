"""
Minimum Description Length Principle (MDLP)
"""

# Guillermo Navas-Palencia <g.navas.palencia@gmail.com>
# Copyright (C) 2020

import numbers

from typing import Self

import numpy as np
import numpy.typing as npt

from scipy import special
from sklearn.base import BaseEstimator
from sklearn.exceptions import NotFittedError
from sklearn.utils import check_array


def _check_parameters(
    min_samples_split: int,
    min_samples_leaf: int,
    max_candidates: int
) -> None:
    if (not isinstance(min_samples_split, numbers.Integral) or
            min_samples_split < 2):
        raise ValueError("min_samples_split must be a positive integer >= 2; "
                         "got {}.".format(min_samples_split))

    if (not isinstance(min_samples_leaf, numbers.Integral) or
            min_samples_leaf < 1):
        raise ValueError("min_samples_leaf must be a positive integer >= 1; "
                         "got {}.".format(min_samples_leaf))

    if not isinstance(max_candidates, numbers.Integral) or max_candidates < 1:
        raise ValueError("max_candidates must be a positive integer >= 1; "
                         "got {}.".format(max_candidates))


class MDLP(BaseEstimator):
    """
    Minimum Description Length Principle (MDLP) discretization algorithm.

    Parameters
    ----------
    min_samples_split : int (default=2)
        The minimum number of samples required to split an internal node.

    min_samples_leaf : int (default=2)
        The minimum number of samples required to be at a leaf node.

    max_candidates : int (default=32)
        The maximum number of split points to evaluate at each partition.

    Notes
    -----
    Implementation of the discretization algorithm in [FI93]. A dynamic
    split strategy based on binning the number of candidate splits [CMR2001]
    is implemented to increase efficiency. For large size datasets, it is
    recommended to use a smaller ``max_candidates`` (e.g. 16) to get a
    significant speed up.

    References
    ----------

    .. [FI93] U. M. Fayyad and K. B. Irani. "Multi-Interval Discretization of
              Continuous-Valued Attributes for Classification Learning".
              International Joint Conferences on Artificial Intelligence,
              13:1022â€“1027, 1993.

    .. [CMR2001] D. M. Chickering, C. Meek and R. Rounthwaite. "Efficient
                 Determination of Dynamic Split Points in a Decision Tree". In
                 Proceedings of the 2001 IEEE International Conference on Data
                 Mining, 91-98, 2001.
    """
    def __init__(
        self,
        min_samples_split: int = 2,
        min_samples_leaf: int = 2,
        max_candidates: int = 32
    ):
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.max_candidates = max_candidates

        # auxiliary
        self._splits = []

        self._is_fitted = None

    def fit(self, x: npt.ArrayLike, y: npt.ArrayLike) -> Self:
        """Fit MDLP discretization algorithm.

        Parameters
        ----------
        x : array-like, shape = (n_samples)
            Data samples, where n_samples is the number of samples.

        y : array-like, shape = (n_samples)
            Target vector relative to x.

        Returns
        -------
        self : MDLP
        """
        return self._fit(x, y)

    def _fit(self, x: npt.ArrayLike, y: npt.ArrayLike) -> Self:
        _check_parameters(**self.get_params())

        x = check_array(x, ensure_2d=False, ensure_all_finite=True)
        y = check_array(y, ensure_2d=False, ensure_all_finite=True)

        idx = np.argsort(x)
        x = x[idx]
        y = y[idx]

        self._recurse(x, y, 0)

        self._is_fitted = True

        return self

    def _recurse(self, x: npt.NDArray, y: npt.NDArray, id: int) -> None:
        u_x = np.unique(x)
        n_x = len(u_x)
        n_y = len(np.bincount(y))

        split = self._find_split(u_x, x, y)

        if split is not None:
            self._splits.append(split)
            t = np.searchsorted(x, split, side="right")

            if not self._terminate(n_x, n_y, y, y[:t], y[t:]):
                self._recurse(x[:t], y[:t], id + 1)
                self._recurse(x[t:], y[t:], id + 2)

    def _find_split(
        self,
        u_x: npt.NDArray,
        x: npt.NDArray,
        y: npt.NDArray
    ) -> float:
        n_x = len(x)
        u_x = np.unique(0.5 * (x[1:] + x[:-1])[(y[1:] - y[:-1]) != 0])

        if len(u_x) > self.max_candidates:
            percentiles = np.linspace(1, 100, self.max_candidates)
            splits = np.percentile(u_x, percentiles)
        else:
            splits = u_x

        max_entropy_gain = 0
        best_split = None

        tt = np.searchsorted(x, splits, side="right")
        for i, t in enumerate(tt):
            samples_l = t >= self.min_samples_leaf
            samples_r = n_x - t >= self.min_samples_leaf

            if samples_l and samples_r:
                entropy_gain = self._entropy_gain(y, y[:t], y[t:])
                if entropy_gain > max_entropy_gain:
                    max_entropy_gain = entropy_gain
                    best_split = splits[i]

        return best_split

    def _entropy(self, x: npt.NDArray) -> float:
        n = len(x)
        ns1 = np.sum(x)
        ns0 = n - ns1
        p = np.array([ns0, ns1]) / n
        return -special.xlogy(p, p).sum()

    def _entropy_gain(
        self,
        y: npt.NDArray,
        y1: npt.NDArray,
        y2: npt.NDArray
    ) -> float:
        n = len(y)
        n1 = len(y1)
        n2 = n - n1
        ent_y = self._entropy(y)
        ent_y1 = self._entropy(y1)
        ent_y2 = self._entropy(y2)
        return ent_y - (n1 * ent_y1 + n2 * ent_y2) / n

    def _terminate(
        self,
        n_x: int,
        n_y: int,
        y: npt.NDArray,
        y1: npt.NDArray,
        y2: npt.NDArray
    ) -> bool:
        splittable = (n_x >= self.min_samples_split) and (n_y >= 2)

        n = len(y)
        n1 = len(y1)
        n2 = n - n1
        ent_y = self._entropy(y)
        ent_y1 = self._entropy(y1)
        ent_y2 = self._entropy(y2)
        gain = ent_y - (n1 * ent_y1 + n2 * ent_y2) / n

        k = len(np.bincount(y))
        k1 = len(np.bincount(y1))
        k2 = len(np.bincount(y2))

        t0 = np.log(3**k - 2)
        t1 = k * ent_y
        t2 = k1 * ent_y1
        t3 = k2 * ent_y2
        delta = t0 - (t1 - t2 - t3)

        return gain <= (np.log(n - 1) + delta) / n or not splittable

    @property
    def splits(self) -> np.ndarray:
        """List of split points

        Returns
        -------
        splits : numpy.ndarray
        """
        if not self._is_fitted:
            raise NotFittedError("This {} instance is not fitted yet. Call "
                                 "'fit' with appropriate arguments."
                                 .format(self.__class__.__name__))

        return np.sort(self._splits)
